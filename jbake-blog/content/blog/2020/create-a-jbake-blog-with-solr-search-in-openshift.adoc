= Create a JBake blog with Solr search in Openshift 
Albert Lacambra Basil 
:jbake-title: Create a JBake blog with Solr search in Openshift 
:description: Create a JBake blog with Solr search in Openshift with Jankins build and automated deployment on push event
:jbake-date: 2020-05-14
:toc:
:jbake-type: post 
:jbake-status: published 
:jbake-tags: solr, openshift, jbake, jenkins 
:doc-id: create-a-jbake-blog-with-solr-search-in-openshift 

toc::[]


== What we are gone a do

Basically the blog is plain html/css/jss without backend. However, I wanted to have some search capabilities so I have added a solr server.

Also the blog should be _code friendly_ so I wanted to write it using *asciidoctor* syntax. 

To convert _adoc_ files to html, I am gone a use link:https://jbake.org/[JBake,window=_blank].

The process to add a new blog entry is as follows:

. Write the articale into a new adoc document
. Push to a git repo
. Build the website with JBake
. Deploy it to an Nginx server
. Index all blog entries with a Solr server

image::/blog/2020/create-a-jbake-blog-with-solr-search-in-openshift/blog-parts.png[width=100%, height=100%, parts]

== Building the website with JBake

To build a JBake site is quite easy. In short you just need to run the following command:

[source, bash]
----
13:49:03 ➜   jbake -i
JBake v2.6.4 (2019-01-21 21:03:37PM) [http://jbake.org]

Base folder structure successfully created.
13:49:08 ➜   ls -la
total 8
drwxr-xr-x    6 albertlacambra  staff   192 May  9 13:49 .
drwxr-xr-x+ 102 albertlacambra  staff  3264 May  9 13:49 ..
drwxr-xr-x    6 albertlacambra  staff   192 May  9 13:49 assets
drwxr-xr-x    4 albertlacambra  staff   128 May  9 13:49 content
-rw-r--r--    1 albertlacambra  staff    64 May  9 13:49 jbake.properties
drwxr-xr-x   12 albertlacambra  staff   384 May  9 13:49 templates
----

Now we have a basic website build. The blog contents are gone a be saved under the folder *content*. My Current structure looks like that:

[source, bash]
----
13:53:34:~/git/lacambra.tech/blog-build ->ls -lR jbake-blog/content/
total 24
-rw-r--r--  1 albertlacambra  staff  216 20 Okt  2019 404.html
-rw-r--r--  1 albertlacambra  staff  883  4 Nov  2019 about.adoc
drwxr-xr-x  4 albertlacambra  staff  128 22 Jan 18:55 blog

jbake-blog/content/blog:
total 0
drwxr-xr-x  26 albertlacambra  staff  832  1 Mär 18:40 2019
drwxr-xr-x  13 albertlacambra  staff  416  8 Mai 16:34 2020

jbake-blog/content//blog/2019:
total 264
-rw-r--r--  1 albertlacambra  staff   636  8 Nov  2019 add-new.certificate-to-be-accepted-maven.adoc
-rw-r--r--  1 albertlacambra  staff  7726 18 Dez 21:39 apache-poi-and-excel-generation-basics.adoc
...

jbake-blog/content//blog/2020:
total 120
-rw-r--r--  1 albertlacambra  staff  5385  3 Feb 22:15 cdi-event-with-jta-transactions.adoc
-rw-r--r--  1 albertlacambra  staff  2005  9 Mai 13:53 create-a-jbake-blog-with-solr-search-in-openshift.adoc
...
----

Just go to link:https://jbake.org/[JBake Official Site,window=_blank] to know more about this amazing tool :)

Now what I am intereested is about to automatize this build process. To do that, I have a created a *jenkins image* and a *jenkins pipeline*

== Create the JBake docker image
We need to create here a jenkins slave image with JBake already installed.

[source, Dockerfile]
----
FROM quay.io/openshift/origin-jenkins-agent-base:v4.0
# This is a base image that install and configures JBake.
# Child Dockerfiles can run command `jbake` to bake or anything else.

# Define environment variables.
ENV BUILD_DATE=05252019
ENV JBAKE_HOME=/opt/jbake
ENV JBAKE_USER=jbake
ENV JBAKE_VERSION=2.6.4
ENV PATH ${JBAKE_HOME}/bin:$PATH

RUN adduser ${JBAKE_USER}

RUN mkdir -p /opt/jbake-${JBAKE_VERSION} && \
    ln -s /opt/jbake-${JBAKE_VERSION} ${JBAKE_HOME} && \
    chown ${JBAKE_USER}:${JBAKE_USER} -R /opt/jbake*

RUN mkdir /opt/jbake-structure && \
    chown ${JBAKE_USER}:${JBAKE_USER} -R /opt/jbake*
    
RUN cd ~ && wget https://dl.bintray.com/jbake/binary/jbake-${JBAKE_VERSION}-bin.zip && \
    unzip ~/jbake-${JBAKE_VERSION}-bin.zip && \
    cd /opt && cp -R ~/jbake-${JBAKE_VERSION}-bin/* ${JBAKE_HOME}/ && \
    rm ~/jbake-${JBAKE_VERSION}-bin.zip && \ 
    rm -rf ~/jbake-${JBAKE_VERSION}-bin

RUN cd /opt

USER ${JBAKE_USER}

CMD jbake -b /opt/jbake-structure
----

== Create a JBake Openshift's ImageStream 
Then we just need to push it to the docker repo our choise (in my case the docker hub) and then create an imagestream with it in Openshift:

[source, bash]
----
14:07:35 ➜ docker push alacambra/jbake

....
lots of blablablabla here....
....

14:14:21 ➜  junk oc import-image  jbake --from=alacambra/jbake:latest --confirm
imagestream.image.openshift.io/jbake imported

Name:			jbake
Namespace:		playground
Created:		Less than a second ago
Labels:			<none>
Annotations:		openshift.io/image.dockerRepositoryCheck=2020-05-09T12:14:31Z
Docker Pull Spec:	docker-registry.default.svc:5000/playground/jbake
Image Lookup:		local=false
Unique Images:		1
Tags:			    1

latest
  tagged from alacambra/jbake:latest

  * alacambra/jbake@sha256:8bc165fcee614dd71b42ab4e5b48d620633d97b6e72f4bdf3057df6d2c828de6
      Less than a second ago

Image Name:	jbake:latest
Docker Image:	alacambra/jbake@sha256:8bc165fcee614dd71b42ab4e5b48d620633d97b6e72f4bdf3057df6d2c828de6
Name:		sha256:8bc165fcee614dd71b42ab4e5b48d620633d97b6e72f4bdf3057df6d2c828de6
Created:	Less than a second ago
Annotations:	image.openshift.io/dockerLayersOrder=ascending
Image Size:	234.4MB in 10 layers

....
lots of further blablablabla here....
....
----

== Register the JBake image as jenikins slave
The last step is to tell openshift that this image is gonna be used as a *jenkins slave*

To do that it is enough to add some special labels to the created image streams:

* *role: jenkins-slave* indicates that this image is to be used by a jenkins slave

* *slave-label: jbake* is the value to be used into the jenkins pipelins to use this slave image.

You can add it through openshift directly or create the imagestream using a template instead to use the _oc import-image_ command.

[source, yaml]
----
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  labels:
    app: [my-app]
    role: jenkins-slave
    slave-label: jbake
  name: jbake
  namespace: blog
spec:
  tags:
    - annotations:
        openshift.io/generated-by: OpenShiftWebConsole
        openshift.io/imported-from: alacambra/jbake
      from:
        kind: DockerImage
        name: alacambra/jbake
      name: latest
      referencePolicy:
        type: Source
----

== Create an Nginx S2i image for the blog

Here we are gonna a create an S2i image, so per each new push a new image will be created

[NOTE] 
====
If you do not know about S2i, visit the official website: link:https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/builds_and_image_streams.html#source-build[Source-to-Image (S2I) Build, window=_blank]
====

The foolowing image is based on link:https://github.com/openshift/source-to-image/blob/master/docs/builder_image.md
[source, Dockerfile]
----

FROM centos:centos7
LABEL maintainer="Albert Lacambra Basil <albert@lacambra.tech>"
ENV NGINX_VERSION=1.6.3

# Set the labels that are used for OpenShift to describe the builder image.
LABEL io.k8s.description="Nginx Webserver" \
    io.k8s.display-name="Nginx 1.6.3" \
    io.openshift.expose-services="8080:http" \
    io.openshift.tags="builder,webserver,html,nginx" \
    # this label tells s2i where to find its mandatory scripts
    # (run, assemble, save-artifacts)
    io.openshift.s2i.scripts-url="image:///usr/libexec/s2i"

RUN yum install -y epel-release && \
    yum install -y --setopt=tsflags=nodocs nginx && \
    yum clean all

# Change the default port for nginx 
# Required if you plan on running images as a non-root user).
RUN sed -i 's/80/8080/' /etc/nginx/nginx.conf

# Copy the S2I scripts to /usr/libexec/s2i since we set the label that way
COPY ./s2i/bin/ /usr/libexec/s2i

ENV NGINX_USER=nginx

RUN chown -R ${NGINX_USER}:${NGINX_USER} /usr/share/nginx
RUN chown -R ${NGINX_USER}:${NGINX_USER} /var/log/nginx
RUN chown -R ${NGINX_USER}:${NGINX_USER} /var/lib/nginx
RUN touch /run/nginx.pid
RUN chown -R ${NGINX_USER}:${NGINX_USER} /run/nginx.pid
RUN chown -R ${NGINX_USER}:${NGINX_USER} /etc/nginx

#Workaround to fix execution with no nginx user...
RUN chmod -R 777 /var/log/nginx
RUN chmod -R 777 /var/lib/nginx
RUN  chmod -R 777 /run/nginx.pid


USER 999
EXPOSE 8080
CMD ["/usr/libexec/s2i/usage"]
----

The next step is to create the s2i scripts. Basically we need to create the _./s2i/bin/assemble_ ans the _./s2i/bin/run_ script 

.assemble
[source, bash]
----
#!/bin/bash -e
if [[ "$1" == "-h" ]]; then
	exec /usr/libexec/s2i/usage
fi

echo "---> Building and installing application from source..."

TRACK=s2ibuild-${RANDOM:0:5}

echo "${TRACK}:here is source"
ls -l /tmp/src/
echo "${TRACK}:source is done"

if [ -f /tmp/src/nginx.conf ]; then
  echo "${TRACK}:added nginx.conf"
  mv /tmp/src/nginx.conf /etc/nginx/nginx.conf
fi
if [ "$(ls -A /tmp/src)" ]; then
  echo "${TRACK}:added rest of files"
  mv /tmp/src/output/* /usr/share/nginx/html/
fi
----

Assamble script basically copies the passed files to the nginx directory:

* *nginx.conf* will be moved to */etc/nginx/nginx.conf* becomming the config file used by nginx
* any file under *output* will be served by added to nginx as static resources. Is in this folder where we need to place the generated website.


.run
[source, bash]
----
exec /usr/sbin/nginx -g "daemon off;"
----

The run command is exactly that. The run command. Is what normlly you will put in you Dockerfile under *CMD ...*

== Create a piple to build JBake and an Nginx image

Basically we need to execute three steps:

. Execute the command _jbake -b jbake-blog/_
. Copy the the generated website into the folder where nginx will start a new buid
. Start an S2i Nginx build

[source, Jenkinsfile]
----
def applicationName = "blog";

pipeline{
    agent {
        label 'jbake'
    }

    stages{
        stage('build-blog') {
            steps{
                sh script: "jbake -b jbake-blog/"
            }
        }
        stage('copy-blog') {
            steps{
                sh script: "cp -Rf jbake-blog/output s2i-nginx/files/ "
            }
        }
        stage('s2i build'){
            steps{
                script{
                    openshift.withCluster(){
                        openshift.withProject(){
                            def build = openshift.selector("bc", applicationName);
                            def startedBuild = build.startBuild("--from-file=\"./s2i-nginx/files\"");
                            startedBuild.logs('-f');
                            echo "${applicationName} build status: ${startedBuild.object().status}";
                        }
                    }
                }
            }
        }
    }
}
----

== Create Solr s2I image

As like the Nginyx image I am gona use S2i:

.Solr Dockerfile
[source, Dockerfile]
----
FROM solr:7.7
MAINTAINER  Albert Lacambra Basil "albert@lacambra.tech"

USER root
ENV STI_SCRIPTS_PATH=/usr/libexec/s2i

LABEL io.k8s.description="Run SOLR search in OpenShift" \
      io.k8s.display-name="SOLR 7.7" \
      io.openshift.expose-services="8983:http" \
      io.openshift.tags="builder,solr,solr7.7" \
      io.openshift.s2i.scripts-url="image:///${STI_SCRIPTS_PATH}"

RUN chgrp -R 0 /opt/solr \
  && chmod -R g+rwX /opt/solr

RUN chgrp -R 0 /opt/docker-solr \
  && chmod -R g+rwX /opt/docker-solr

COPY ./s2i/bin/. ${STI_SCRIPTS_PATH}
RUN chmod -R a+rx ${STI_SCRIPTS_PATH}

USER 8983
----

This image is based on link:https://github.com/dudash/openshift-docker-solr[window=_blank]

=== Solr configuration
We need also to configure the schema that we are gone to use for our documents:

.schema.xml
[source, xml]
----
<?xml version="1.0" encoding="UTF-8" ?>

<schema name="post" version="1.1">
    <uniqueKey>id</uniqueKey>

    <fieldType name="string" class="solr.StrField"/>
    <fieldType name="text" class="solr.TextField">
        <analyzer type="index">
            <tokenizer class="solr.StandardTokenizerFactory"/>
            <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
        <analyzer type="query">
            <tokenizer class="solr.StandardTokenizerFactory"/>
            <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
    </fieldType>

    <field name="id" type="string" indexed="true" stored="true" multiValued="false" required="true"/>
    <field name="url" type="string" indexed="true" stored="true"/>
    <field name="title" type="text" indexed="true" stored="true"/>
    <field name="description" type="text" indexed="true" stored="true"/>
    <field name="reducedText" type="text" indexed="true" stored="true"/>
    <field name="text" type="text" indexed="true" stored="true"/>
    <field name="date" type="text" indexed="true" stored="true"/>

</schema>
----

More about configuring solr: link:https://lucene.apache.org/solr/guide/7_0/solr-configuration-files.html[solr-configuration-files, window=_blank]

=== Solr S2i scripts

as with nginx we need to create the **assamble** and **run** script.

.assamble
[source, bash]
----
#!/bin/bash
SOURCE_FILE_DIR=/tmp/src

if [ -z "${SOLR_CONF_DIR}" ]; then
  SOLR_CONF_DIR="${SOLR_HOME:-/opt/solr/server/solr}/configsets"
fi

echo "==========================================================="
echo "Source Repo Files"
echo "-----------------------------------------------------------"
ls -al ${SOURCE_FILE_DIR}
echo "==========================================================="
echo

echo "Copying SOLR conf files from ${SOURCE_FILE_DIR}/. to ${SOLR_CONF_DIR} ..."
cp -Rf ${SOURCE_FILE_DIR}/. ${SOLR_CONF_DIR}
echo "Copied SOLR conf files."
echo
----

.run [source, bash]
----
#!/bin/bash -e
# The run script executes the SOLR server
# For more information see the documentation:
# https://docs.openshift.org/latest/creating_images/s2i.html#creating-images-s2i

# Override this env variable to change the name of your core.
if [ -z "${CORE_NAME}" ]; then
  CORE_NAME=blog-solr
fi

if [ -z "${SOLR_CONF_DIR}" ]; then
  SOLR_CONF_DIR="${SOLR_HOME:-/opt/solr/server/solr}/configsets"
fi

if [ -z "${CONF_DIR}" ]; then
  CONF_DIR="${SOLR_CONF_DIR}/${CORE_NAME}"
fi

if [ -z "${SOLR_HOME}" ]; then
  CORES_DIR="/opt/solr/server/solr/mycores"
else
  CORES_DIR="${SOLR_HOME}"
fi

CORE_DIR="${CORES_DIR}/${CORE_NAME}"

# Use scripts from the official docker-solr image
# https://github.com/docker-solr/docker-solr/tree/master/scripts
echo "Working from; $(pwd)"
echo

if [[ ! -d ${CORE_DIR} ]]; then
  echo "====================================================================="
  echo "Pre-Creating SOLR Core:"
  echo "---------------------------------------------------------------------"
  echo "Core Name: ${CORE_NAME}" 
  echo "Configuration: ${CONF_DIR}"
  echo
  echo "By default cores are created in either the SOLR_HOME directory"
  echo "or the /opt/solr/server/solr/mycores directory."
  echo
  echo "If the SOLR_HOME env variable is defined the core will be created"
  echo "in that directory.  Otherwise the core will be created in"
  echo "/opt/solr/server/solr/mycores"
  echo "====================================================================="
  
  solr-precreate ${CORE_NAME} ${CONF_DIR}
else
  echo "====================================================================="
  echo "Updating SOLR Configuration Files:"
  echo "---------------------------------------------------------------------"
  echo "Core Name: ${CORE_NAME}" 
  echo "Configuration Source: ${CONF_DIR}"
  echo "Configuration Destination: ${CORES_DIR}"
  echo
  echo "By default cores are created in either the SOLR_HOME directory"
  echo "or the /opt/solr/server/solr/mycores directory."
  echo
  echo "If the SOLR_HOME env variable is defined the core will be created"
  echo "in that directory.  Otherwise the core will be created in"
  echo "/opt/solr/server/solr/mycores"
  echo "====================================================================="
  
  cp -r "${CONF_DIR}/" "${CORES_DIR}"
  exec solr -f
fi
----

Basically we copy all config files under ${SOURCE_FILE_DIR} under the Solr configuration directory ${SOLR_CONF_DIR}. THe only file I have added to the default config is the **schema.xml**. The rest are just defaults.

== Create java code to index blog entries

* *SolrClientProvider*: It creates a solr client.
* *Indexer*: Takes files under a given directory and create a SolrDocument to be indexed. Basically it makes the step *(adoc) -[:to]-> (solr)*
* *ParsedDocument*: It pases each file and extracts the headers, the body and the filename. Those fileds are being added to the SolrDocuemnt.
* *Cli*: A class providing endpoints to perform actions like _delete, reindex, ..._ using link:https://github.com/tomitribe/crest#example[Crest from tomitribe,window=_blank]

[source, java]
----
package tech.lacambra.blog.solr_indexing;

import org.apache.solr.client.solrj.impl.HttpSolrClient;

public class SolrClientProvider {

  public static HttpSolrClient getClient() {
    final String solrUrl = "http://solr-blog-blog.apps.oc.lacambra/solr";
    return new HttpSolrClient.Builder(solrUrl)
        .withConnectionTimeout(10000)
        .withSocketTimeout(60000)
        .build();
  }
}
----

[source, java]
----
package tech.lacambra.blog.solr_indexing;

import org.apache.solr.client.solrj.SolrServerException;
import org.apache.solr.client.solrj.impl.HttpSolrClient;
import org.apache.solr.common.SolrInputDocument;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.logging.Logger;

public class Indexer {
  private static final Logger LOGGER = Logger.getLogger(Indexer.class.getName());
  private HttpSolrClient client;

  public static void main(String[] args) throws IOException {

    if (args.length < 1) {
      throw new RuntimeException("No content path given");
    }

    try (HttpSolrClient client = SolrClientProvider.getClient()) {
      Indexer indexer = new Indexer(client);
      indexer.indexAll(args[0]);
    } catch (Exception e) {
      e.printStackTrace();
    }
  }

  public Indexer(HttpSolrClient client) {
    this.client = client;
  }

  public void indexAll(String path) {
    Path contentPath = Paths.get(path);
    try {
      Files.walk(contentPath, 1)
          .filter(p -> !p.equals(contentPath) && Files.isDirectory(p))
          .flatMap(p -> {
            try {
              return Files.walk(p, 1);
            } catch (IOException e) {
              throw new RuntimeException(e);
            }
          })
          .filter(p -> Files.isRegularFile(p))
          .map(p -> {

            try {
              return parseAdocText(p);
            } catch (IOException e) {
              throw new RuntimeException(e);
            }

          })
          .filter(ParsedDocument::isPosted)
          .forEach(this::indexDoc);
    } catch (IOException e) {
      LOGGER.info("[indexAll] Error: " + e.getMessage());
    }
  }

  private void indexDoc(ParsedDocument parsedDocument) {

    final SolrInputDocument doc = new SolrInputDocument();

    String id = parsedDocument.getHeaderValue("doc-id").orElseThrow(() -> new RuntimeException("Id must be given for:" + parsedDocument.getUrl()));
    doc.addField("id", id);
    doc.addField("url", parsedDocument.getUrl());
    doc.addField("title", parsedDocument.getHeaderValue("jbake-title").orElse(""));
    doc.addField("description", parsedDocument.getHeaderValue("description").orElse(""));
    doc.addField("reducedText", parsedDocument.getTextResume());
    doc.addField("text", parsedDocument.getBodyText());
    doc.addField("date", parsedDocument.getDisplayDate());

    String collection = "blog-solr";

    try {
      client.add(collection, doc);
      client.commit(collection);
      LOGGER.info("[indexDoc] Indexed document " + id);

    } catch (SolrServerException | IOException e) {
      e.printStackTrace();
    }


  }

  private ParsedDocument parseAdocText(Path path) throws IOException {

    ParsedDocument parsedDocument = new ParsedDocument(path.toString());
    Files.readAllLines(path).forEach(parsedDocument::parseLine);

    return parsedDocument;
  }
}

----

[source, java]
----
package tech.lacambra.blog.solr_indexing;

import java.util.*;
import java.util.stream.Collectors;
import java.util.stream.Stream;

public class ParsedDocument {

  private Map<String, String> headers;
  private List<String> body;
  private String fileName;

  public ParsedDocument(String fileName) {
    this.fileName = fileName.replace(".adoc", ".html");
    headers = new HashMap<>();
    body = new ArrayList<>();
  }

  public ParsedDocument parseLine(String line) {

    line = line.trim();

    if (line.startsWith(":")) {

      String headerName = parseHeaderName(line);
      String headerValue = parseHeaderValue(line);
      headers.put(headerName, headerValue);

    } else {

      body.add(line);

    }

    return this;
  }

  String parseHeaderName(String header) {

    header = header.trim();

    int index = header.indexOf(":", 1);
    String headerName = header.substring(1, index);

    return headerName.trim();
  }

  String parseHeaderValue(String header) {

    header = header.trim();

    int index = header.indexOf(":", 1);
    String headerValue = header.substring(index + 1);

    return headerValue.trim();
  }

  public List<String> getBodyLines() {
    return new ArrayList<>(body);
  }

  public String getBodyText() {

    Stream.of("jbake-title", "description")
        .map(this::getHeaderValue)
        .filter(Optional::isPresent)
        .map(Optional::get)
        .forEach(body::add);

    return String.join("\n", body);
  }

  public Map<String, String> getHeaders() {
    return new HashMap<>(headers);
  }

  public String getTextResume() {
    return body.stream().limit(5).collect(Collectors.joining("\n"));
  }

  public Optional<String> getHeaderValue(String headerName) {
    return Optional.ofNullable(headers.get(headerName));
  }

  public String getUrl() {
    return fileName.substring(fileName.indexOf("content") + "content".length());
  }

  public boolean isPosted() {
    return headers.getOrDefault("jbake-type", "").equals("post");
  }

  public String getDisplayDate() {
    return headers.getOrDefault("jbake-date", "");
  }

  public String getFileName() {
    return fileName;
  }
}

----

[source, java]
----
package tech.lacambra.blog.solr_indexing;

import org.apache.solr.client.solrj.SolrQuery;
import org.apache.solr.client.solrj.impl.HttpSolrClient;
import org.apache.solr.client.solrj.response.QueryResponse;
import org.apache.solr.client.solrj.response.UpdateResponse;
import org.tomitribe.crest.Main;
import org.tomitribe.crest.api.Command;
import org.tomitribe.crest.api.Default;
import org.tomitribe.crest.api.Option;
import org.tomitribe.crest.environments.SystemEnvironment;

import java.util.logging.Logger;


public class Cli {

  private static final Logger LOGGER = Logger.getLogger(Cli.class.getName());
  private String collection = "blog-solr";

  @Command("check")
  public void check() {
    LOGGER.info("[check] OK!");
  }


  @Command("delete")
  public void delete(@Option("id") @Default("") String id) {

    if (id.isEmpty()) {
      deleteAll();
    } else {
      deleteId(id);
    }
  }

  @Command("reindex")
  public void reindex(@Option("path") @Default("") String path) {

    if (path.isEmpty()) {
      throw new RuntimeException("invalid path=" + path);
    }

    deleteAll();

    try (HttpSolrClient client = SolrClientProvider.getClient()) {
      Indexer indexer = new Indexer(client);
      indexer.indexAll(path);
    } catch (Exception e) {
      e.printStackTrace();
    }


  }

  public static void main(String[] args) throws Exception {
    Main main = new Main(Cli.class);
    main.main(new SystemEnvironment(), args);
  }

  private void deleteAll() {
    try (HttpSolrClient client = SolrClientProvider.getClient()) {

      if ("".isEmpty()) {
        SolrQuery q = new SolrQuery("*:*");
        q.addField("id");

        QueryResponse r = client.query(collection, q);

        while (!r.getResults().isEmpty()) {
          r.getResults().stream()
              .map(d -> d.get("id"))
              .forEach(id -> {
                try {
                  UpdateResponse ur = client.deleteById(collection, (String) id);
                  client.commit(collection);
                  LOGGER.info("[main] Deleted document " + id);

                } catch (Exception e) {
                  LOGGER.info("[main] Error deleting " + id + " : " + e.getMessage());
                }
              });

          r = client.query(collection, q);
        }
      }


    } catch (Exception e) {
      e.printStackTrace();
    }
  }

  private void deleteId(String id) {
    try (HttpSolrClient client = SolrClientProvider.getClient()) {

      UpdateResponse ur = client.deleteById(collection, (String) id);
      client.commit(collection);
      LOGGER.info("[main] Deleted document " + id);

    } catch (Exception e1) {
      e1.printStackTrace();
    }
  }
}
----

== Create a pipeline to index blog entries

This pipeline will just execute the *Indexer*. Then all the _adocs_ will be reindexed to *Solr*.

[source, Jenkinsfile]
----
def applicationName = "solr-indexing";

pipeline{
    agent {
        label 'maven'
    }

    stages{
        stage('build-solr-indexing') {
            steps{
                sh script: "cd ${applicationName} && mvn clean package"
            }
        }
        stage('index-entries') {
            steps{
                sh script: "cd ${applicationName} && mvn exec:java -Dexec.mainClass=tech.lacambra.blog.solr_indexing.Indexer -Dexec.args=\"../jbake-blog/content/blog/\""
            }
        }
    }
}
----

== Integrate Solr with JBake

Once everytging is configured and working, I need to be able to call solr from the website.

.solr-search.js
[source, javascript]
----
const url = "/search/blog-solr/select?q="

const search = (searchId) => {

    //Pick the query from the search input box and create the search query
    const q = "*" + document.querySelector(`#${searchId}`).value.trim().replace(/[ ]+/g, "*%20%26%26%20*") + "*";
    const select = `${url}${q}`;
    
    fetch(select)
        .then(response => response.json())
        .then(j => {

            //Prepare the results section
            let main = document.querySelector("#main");
            main.innerHTML = "";

            //Show "nothing found" error
            if (j["response"]["docs"].length === 0) {
                main.innerHTML = `<article class="post"><header><div class="title">Nothing found for ${q}</div></header></article>`;
            }

            //Load results to main section
            j["response"]["docs"].forEach(entry => {
                const template = document.querySelector('#search-result');
                let rendered = Mustache.render(template.innerHTML, entry);
                let div = document.createElement("div");
                div.innerHTML = rendered;
                main.appendChild(div);
            });
        });

    return false;
}
----

A solr response will look like:
[source, json]
----
{
    "responseHeader": {
        "status": 0,
        "QTime": 3,
        "params": {
            "q": "*:*",
            "_": "1572110726768"
        }
    },
    "response": {
        "numFound": 1,
        "start": 0,
        "docs": [
            {
                "id": "...",
                "url": "...",
                "title": "...",
                "description": "...",
                "reducedText": "...",
                "text": "...",
                "date": "..."
            },
        ]
    }
}
----

In jabke you can load the javascripts file in the footer.ftl (it will depends on your tempaltes though):

.footer.ftl
[source, html]
----
<script src="<#if (content.rootpath)??>${content.rootpath}<#else></#if>js/solr-search.js"></script>
----

The template for the search results is created in *menu.ftl*

.menu.ftl
[source, html]
----
<template id="search-result">
    <article class="post">
        <header>
            <div class="title">
                <h2><a href="{{url}}">{{title}}</a></h2>
                <p>{{description}}</p>
            </div>
            <div class="meta">
                <time class="published"
                    datetime='{{date}}'>
                    {{date}}</time>
                    <div class="published eta"></div>
                <span class="author"/><span class="name">Albert Lacambra Basil</span><img src="../../img/main/avatar.png" alt="Albert Lacambra Basil" /></span>
            </div>
        </header>
    </article>
</template>
----

and finally bind the search function to the search menu. 

.menu.ftl
[source, html]
----
<li class="search">
    <a class="fa-search" href="#search">Search</a>
    <form id="search" onsubmit="return search('search-query');">
        <input id="search-query" type="text" name="q" placeholder="Search" />
        <input type="hidden" name="q" value="site:${config.site_host}">
    </form>
</li>
----

And taht's all!s



